{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\"><strong><em>MERCADO LIBRE SCRAPER</em></strong></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter between ' ' the item on which you want to obtain the data in Mercado Libre.\n",
    "#### You will receive a csv file with that name with all the information from your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 'posters del 89 de Union' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'posters del 89 de Union'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Initial URL for web scraping\n",
    "siguiente = f'https://listado.mercadolibre.com.ar/{item}'\n",
    "\n",
    "\n",
    "# Lists to store the extracted data\n",
    "lista_titulos = []\n",
    "lista_url = []\n",
    "lista_precios = []\n",
    "\n",
    "# Loop to iterate over all result pages\n",
    "while True:\n",
    "    # Perform an HTTP GET request to the URL\n",
    "    r = requests.get(siguiente)\n",
    "    # Check if the request was successful\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        # Create a BeautifulSoup object from the response content\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # Create a DOM object from the response content\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        # Find all h2 elements with the specified class and extract the text from each one\n",
    "        titulos = soup.find_all('h2', attrs={\"class\": \"ui-search-item__title shops__item-title\"})\n",
    "\n",
    "        # Replace all commas with spaces in each title\n",
    "        titulos = [i.text.replace(',', ' ') for i in titulos]\n",
    "\n",
    "        # Add the titulos to the list of titulos\n",
    "        lista_titulos.extend(titulos)\n",
    "\n",
    "        # Find all elements 'a' with the specified class and extract the value of the 'href' attribute from each one\n",
    "        urls = soup.find_all('a', attrs={\"class\": \"ui-search-item__group__element shops__items-group-details ui-search-link\"})\n",
    "        urls = [i.get('href') for i in urls]\n",
    "\n",
    "        # Add the URLs to the list of URLs\n",
    "        lista_url.extend(urls)\n",
    "        \n",
    "        # Find all elements 'div' with the specified class and extract the text from each one\n",
    "        precio = soup.find_all('div', attrs={\"class\": \"ui-search-price__second-line shops__price-second-line\"})\n",
    "\n",
    "        # Use a regular expression to extract only digits and the decimal point from each price text\n",
    "        precio = [re.search(r'\\d+\\.\\d+', i.text).group() if re.search(r'\\d+\\.\\d+', i.text) else '' for i in precio]\n",
    "        \n",
    "        # Add the prices to the list of prices\n",
    "        lista_precios.extend(precio)\n",
    "        \n",
    "        # Find the 'span' element with the specified class and extract the text if it exists\n",
    "        botoninicial_element = soup.find('span', attrs={\"class\": \"andes-pagination__link\"})\n",
    "        if botoninicial_element:\n",
    "            botoninicial = botoninicial_element.text\n",
    "            # Convert the text to an integer\n",
    "            botoninicial = int(botoninicial)\n",
    "        else:\n",
    "            # Handle the case where the element is not found (e.g., set a default value or break out of the loop)\n",
    "            botoninicial = 0  # You can set a default value or handle this as needed\n",
    "\n",
    "        # Find the 'li' element with the specified class and extract the text if it exists\n",
    "        can_element = soup.find('li', attrs={\"class\": \"andes-pagination__page-count\"})\n",
    "        if can_element:\n",
    "            # Split the text into words and take the second word (the total number of pages)\n",
    "            can = int(can_element.text.split(\" \")[1])\n",
    "        else:\n",
    "            # Handle the case where the element is not found (e.g., set a default value or break out of the loop)\n",
    "            can = 0  # You can set a default value or handle this as needed\n",
    "\n",
    "        # Check if we have reached the last page\n",
    "        if botoninicial == can:\n",
    "            break\n",
    "\n",
    "        # Use XPath to find the 'a' element that represents the \"Next\" button and extract the value of the href attribute\n",
    "        next_button_elements = dom.xpath('//div[@class=\"ui-search-pagination shops__pagination-content\"]//ul/li[contains(@class,\"--next\")]/a')\n",
    "\n",
    "        # Check if there are any matching elements\n",
    "        if next_button_elements:\n",
    "            # If there are matching elements, extract the href attribute\n",
    "            siguiente = next_button_elements[0].get('href')\n",
    "        else:\n",
    "            # If there are no matching elements, break out of the loop\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # If the request was not successful, break out of the loop\n",
    "        break\n",
    "\n",
    "# Open a CSV file in write mode and UTF-8 encoding.\n",
    "with open(f'{item}.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    # Create a CSV writer with non-numeric quoting\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_NONNUMERIC)  \n",
    "    # Write the header row to the CSV file\n",
    "    writer.writerow(['Titulo', 'Precio', 'URL'])\n",
    "\n",
    "    # Iterate over the lists of titles, prices, and URLs and write each row to the CSV file\n",
    "    for titulo, precio, url in zip(lista_titulos, lista_precios, lista_url):\n",
    "        writer.writerow([titulo, precio, url])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
